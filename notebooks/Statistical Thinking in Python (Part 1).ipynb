{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical exploratory data analysis\n",
    "\n",
    "Before diving into sophisticated statistical inference techniques, we should first explore our data by plotting them and computing simple summary statistics. This process, called **exploratory data analysis**, is a crucial first step in statistical analysis of data.\n",
    "\n",
    "## Introduction to Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis is the process of organizing, plo!ing, and summarizing a data set\n",
    "\n",
    ">“Exploratory data analysis can never be the\n",
    "whole story, but nothing else can serve as the\n",
    "foundation stone. ” > ~ John Tukey\n",
    "\n",
    "### Tukey's comments on EDA\n",
    "\n",
    "* Exploratory data analysis is detective work.\n",
    "* There is no excuse for failing to plot and look.\n",
    "* The greatest value of a picture is that it forces us to notice what we never expected to see.\n",
    "* It is important to understand what you can do before you learn how to measure how well you seem to have done it.\n",
    "\n",
    "> If you don't have time to do EDA, you really don't have time to do hypothesis tests. And you should always do EDA first.\n",
    "\n",
    "### Advantages of graphical EDA\n",
    "\n",
    "* It often involves converting tabular data into graphical form.\n",
    "* If done well, graphical representations can allow for more rapid interpretation of data.\n",
    "* There is no excuse for neglecting to do graphical EDA.\n",
    "\n",
    "> While a good, informative plot can sometimes be the end point of an analysis, it is more like a beginning: it helps guide you in the quantitative statistical analyses that come next.\n",
    "\n",
    "## Plotting a histogram\n",
    "\n",
    "### Plotting a histogram of iris data\n",
    "\n",
    "We will use a classic data set collected by botanist Edward Anderson and made famous by Ronald Fisher, one of the most prolific statisticians in history. Anderson carefully measured the anatomical properties of samples of three different species of iris, Iris setosa, Iris versicolor, and Iris virginica. The full data set is [available as part of scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). Here, you will work with his measurements of petal length.\n",
    "\n",
    "We will plot a histogram of the petal lengths of his 50 samples of Iris versicolor using matplotlib/seaborn's default settings. \n",
    "\n",
    "The subset of the data set containing the Iris versicolor petal lengths in units of centimeters (cm) is stored in the NumPy array `versicolor_petal_length`.\n",
    "\n",
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set default Seaborn style\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor_petal_length = np.array([4.7, 4.5, 4.9, 4. , 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4. ,\n",
    "       4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4. , 4.9, 4.7, 4.3, 4.4,\n",
    "       4.8, 5. , 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1,\n",
    "       4. , 4.4, 4.6, 4. , 3.3, 4.2, 4.2, 4.2, 4.3, 3. , 4.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of versicolor petal lengths\n",
    "plt.hist(versicolor_petal_length)\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"petal length (cm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the number of bins in a histogram\n",
    "\n",
    "The histogram we just made had ten bins. This is the default of matplotlib. \n",
    "\n",
    ">Tip: The \"square root rule\" is a commonly-used rule of thumb for choosing number of bins: choose the number of bins to be the square root of the number of samples. \n",
    "\n",
    "We will plot the histogram of _Iris versicolor petal lengths_ again, this time using the square root rule for the number of bins. You specify the number of bins using the `bins` keyword argument of `plt.hist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of data points: n_data\n",
    "n_data = len(versicolor_petal_length)\n",
    "\n",
    "# Number of bins is the square root of number of data points: n_bins\n",
    "n_bins = np.sqrt(n_data)\n",
    "\n",
    "# Convert number of bins to integer: n_bins\n",
    "n_bins = int(n_bins)\n",
    "\n",
    "# Plot the histogram\n",
    "_ = plt.hist(versicolor_petal_length, bins=n_bins)\n",
    "\n",
    "# Label axes\n",
    "_ = plt.xlabel('petal length (cm)')\n",
    "_ = plt.ylabel('count')\n",
    "\n",
    "# Show histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all data: Bee swarm plots\n",
    "\n",
    "### Bee swarm plot\n",
    "\n",
    "We will make a bee swarm plot of the iris petal lengths. The x-axis will contain each of the three species, and the y-axis the petal lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_petal_lengths = pd.read_csv(\"../datasets/iris_petal_lengths.csv\")\n",
    "iris_petal_lengths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_petal_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_petal_lengths.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bee swarm plot with Seaborn's default settings\n",
    "_ = sns.swarmplot(data=iris_petal_lengths, x=\"species\", y=\"petal length (cm)\")\n",
    "\n",
    "# Label the axes\n",
    "_ = plt.xlabel(\"species\")\n",
    "_ = plt.ylabel(\"petal length (cm)\")\n",
    "# Show the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a bee swarm plot\n",
    "\n",
    "* _I. virginica_ petals tend to be the longest, and _I. setosa_ petals tend to be the shortest of the three species.\n",
    "\n",
    "> Note: Notice that we said **\"tend to be.\"** Some individual _I. virginica_ flowers may be shorter than individual _I. versicolor_ flowers. It is also possible that an individual _I. setosa_ flower may have longer petals than in individual _I. versicolor_ flower, though this is highly unlikely, and was not observed by Anderson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all data: ECDFs\n",
    "\n",
    "> Note: Empirical cumulative distribution function (ECDF)\n",
    "\n",
    "### Computing the ECDF\n",
    "\n",
    "We will write a function that takes as input a 1D array of data and then returns the `x` and `y` values of the ECDF.\n",
    "\n",
    "> Important: ECDFs are among the most important plots in statistical analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF: x\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF: y\n",
    "    y = np.arange(1, n+1) / n\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ECDF\n",
    "\n",
    "We will now use `ecdf()` function to compute the ECDF for the petal lengths of Anderson's _Iris versicolor_ flowers. We will then plot the ECDF.\n",
    "\n",
    "> Warning: `ecdf()` function returns two arrays so we will need to unpack them. An example of such unpacking is `x, y = foo(data)`, for some function `foo()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECDF for versicolor data: x_vers, y_vers\n",
    "x_vers, y_vers = ecdf(versicolor_petal_length)\n",
    "\n",
    "# Generate plot\n",
    "_ = plt.plot(x_vers, y_vers, marker=\".\", linestyle=\"none\")\n",
    "\n",
    "# Label the axes\n",
    "_ = plt.xlabel(\"versicolor petal length, (cm)\")\n",
    "_ = plt.ylabel(\"ECDF\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of ECDFs\n",
    "\n",
    "ECDFs also allow us to compare two or more distributions ***(though plots get cluttered if you have too many)***. Here, we will plot ECDFs for the petal lengths of all three iris species. \n",
    "\n",
    "> Important: we already wrote a function to generate ECDFs so we can put it to good use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_petal_length = iris_petal_lengths[\"petal length (cm)\"][iris_petal_lengths.species == \"setosa\"]\n",
    "versicolor_petal_length = iris_petal_lengths[\"petal length (cm)\"][iris_petal_lengths.species == \"versicolor\"]\n",
    "virginica_petal_length = iris_petal_lengths[\"petal length (cm)\"][iris_petal_lengths.species == \"virginica\"]\n",
    "setosa_petal_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECDFs\n",
    "x_set, y_set = ecdf(setosa_petal_length)\n",
    "x_vers, y_vers = ecdf(versicolor_petal_length)\n",
    "x_virg, y_virg = ecdf(virginica_petal_length)\n",
    "\n",
    "\n",
    "# Plot all ECDFs on the same plot\n",
    "_ = plt.plot(x_set, y_set, marker=\".\", linestyle=\"none\")\n",
    "_ = plt.plot(x_vers, y_vers, marker=\".\", linestyle=\"none\")\n",
    "_ = plt.plot(x_virg, y_virg, marker=\".\", linestyle=\"none\")\n",
    "\n",
    "# Annotate the plot\n",
    "plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')\n",
    "_ = plt.xlabel('petal length (cm)')\n",
    "_ = plt.ylabel('ECDF')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The ECDFs expose clear differences among the species. Setosa is much shorter, also with less absolute variability in petal length than versicolor and virginica.\n",
    "\n",
    "## Onward toward the whole story!\n",
    "\n",
    "> Important: “Exploratory data analysis can never be the\n",
    "whole story, but nothing else can serve as the\n",
    "foundation stone.”\n",
    " —John Tukey\n",
    "\n",
    "# Quantitative exploratory data analysis\n",
    "\n",
    "We will compute useful summary statistics, which serve to concisely describe salient features of a dataset with a few numbers.\n",
    " \n",
    "## Introduction to summary statistics: The sample mean and median\n",
    "\n",
    "$$\n",
    "mean = \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "> ### Outliers\n",
    "● Data points whose value is far greater or less than\n",
    "most of the rest of the data\n",
    "\n",
    "\n",
    "> ### The median\n",
    "● The middle value of a data set\n",
    "\n",
    "> Note: An outlier can significantly affect the value of the mean, but not the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing means\n",
    "\n",
    "The mean of all measurements gives an indication of the typical magnitude of a measurement. It is computed using `np.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean: mean_length_vers\n",
    "mean_length_vers = np.mean(versicolor_petal_length)\n",
    "\n",
    "# Print the result with some nice formatting\n",
    "print('I. versicolor:', mean_length_vers, 'cm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentiles, outliers, and box plots\n",
    "\n",
    "### Computing percentiles\n",
    "\n",
    " We will compute the percentiles of petal length of _Iris versicolor_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify array of percentiles: percentiles\n",
    "percentiles = np.array([2.5, 25, 50, 75, 97.5])\n",
    "\n",
    "# Compute percentiles: ptiles_vers\n",
    "ptiles_vers = np.percentile(versicolor_petal_length, percentiles)\n",
    "\n",
    "# Print the result\n",
    "ptiles_vers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing percentiles to ECDF\n",
    "\n",
    "To see how the percentiles relate to the ECDF, we will plot the percentiles of _Iris versicolor_ petal lengths on the ECDF plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ECDF\n",
    "_ = plt.plot(x_vers, y_vers, '.')\n",
    "_ = plt.xlabel('petal length (cm)')\n",
    "_ = plt.ylabel('ECDF')\n",
    "\n",
    "# Overlay percentiles as red diamonds.\n",
    "_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',\n",
    "         linestyle=\"none\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-whisker plot\n",
    "\n",
    "> Warning: Making a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine.\n",
    "\n",
    "We will Make a box plot of the iris petal lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plot with Seaborn's default settings\n",
    "_ = sns.boxplot(data=iris_petal_lengths, x=\"species\", y=\"petal length (cm)\")\n",
    "\n",
    "# Label the axes\n",
    "_ = plt.xlabel(\"species\")\n",
    "_ = plt.ylabel(\"petal length (cm)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and standard deviation\n",
    "\n",
    "> ### Variance\n",
    "● The mean squared distance of the data from their\n",
    "mean\n",
    "\n",
    "> Tip: Variance; nformally, a measure of the spread of data\n",
    "> $$\n",
    "variance = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "> ### standard Deviation\n",
    "$$\n",
    "std = \\sqrt {\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "### Computing the variance\n",
    "\n",
    "we will explicitly compute the variance of the petal length of _Iris veriscolor_, we will then use `np.var()` to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of differences to mean: differences\n",
    "differences = versicolor_petal_length-np.mean(versicolor_petal_length)\n",
    "\n",
    "# Square the differences: diff_sq\n",
    "diff_sq = differences**2\n",
    "\n",
    "# Compute the mean square difference: variance_explicit\n",
    "variance_explicit = np.mean(diff_sq)\n",
    "\n",
    "# Compute the variance using NumPy: variance_np\n",
    "variance_np = np.var(versicolor_petal_length)\n",
    "\n",
    "# Print the results\n",
    "print(variance_explicit, variance_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard deviation and the variance\n",
    "\n",
    "the standard deviation is the square root of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the variance: variance\n",
    "variance = np.var(versicolor_petal_length)\n",
    "\n",
    "# Print the square root of the variance\n",
    "print(np.sqrt(variance))\n",
    "\n",
    "# Print the standard deviation\n",
    "print(np.std(versicolor_petal_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and the Pearson correlation coefficient\n",
    "\n",
    "> ### Covariance\n",
    "● A measure of how two quantities vary together\n",
    "> $$\n",
    "covariance = \\frac{1}{n} \\sum_{i=1}^{n} (x_i\\ \\bar{x})\\ (y_i \\ - \\bar{y})\n",
    "$$\n",
    "\n",
    "> ### Pearson correlation coefficient\n",
    "> $$\n",
    "\\rho = Pearson\\ correlation = \\frac{covariance}{(std\\ of\\ x)\\ (std\\ of\\ y)} = \\frac{variability\\ due\\ to\\ codependence}{independent variability}\n",
    "$$\n",
    "\n",
    "### Scatter plots\n",
    "\n",
    "When we made bee swarm plots, box plots, and ECDF plots in previous exercises, we compared the petal lengths of different species of _iris_. But what if we want to compare two properties of a single species? This is exactly what we will do, we will make a **scatter plot** of the petal length and width measurements of Anderson's _Iris versicolor_ flowers. \n",
    "\n",
    "> Important: If the flower scales (that is, it preserves its proportion as it grows), we would expect the length and width to be correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versicolor_petal_width = np.array([1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1. , 1.3, 1.4, 1. , 1.5, 1. ,\n",
    "       1.4, 1.3, 1.4, 1.5, 1. , 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4,\n",
    "       1.4, 1.7, 1.5, 1. , 1.1, 1. , 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3,\n",
    "       1.3, 1.2, 1.4, 1.2, 1. , 1.3, 1.2, 1.3, 1.3, 1.1, 1.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot\n",
    "_ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker=\".\", linestyle=\"none\")\n",
    "\n",
    "\n",
    "# Label the axes\n",
    "_ = plt.xlabel(\"petal length, (cm)\")\n",
    "_ = plt.ylabel(\"petal length, (cm)\")\n",
    "\n",
    "# Show the result\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: we see some correlation. Longer petals also tend to be wider.\n",
    "\n",
    "### Computing the covariance\n",
    "\n",
    "The covariance may be computed using the Numpy function `np.cov()`. For example, we have two sets of data $x$ and $y$, `np.cov(x, y)` returns a 2D array where entries `[0,1`] and `[1,0]` are the covariances. Entry `[0,0]` is the variance of the data in `x`, and entry `[1,1]` is the variance of the data in `y`. This 2D output array is called the **covariance matrix**, since it organizes the self- and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix: covariance_matrix\n",
    "covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)\n",
    "\n",
    "# Print covariance matrix\n",
    "print(covariance_matrix)\n",
    "\n",
    "# Extract covariance of length and width of petals: petal_cov\n",
    "petal_cov = covariance_matrix[0,1]\n",
    "\n",
    "# Print the length/width covariance\n",
    "\n",
    "print(petal_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Pearson correlation coefficient\n",
    "\n",
    "the Pearson correlation coefficient, also called the **Pearson r**, is often easier to interpret than the covariance. It is computed using the `np.corrcoef()` function. Like `np.cov(`), it takes two arrays as arguments and returns a 2D array. Entries `[0,0]` and `[1,1]` are necessarily equal to `1`, and the value we are after is entry `[0,1]`.\n",
    "\n",
    "We will write a function, `pearson_r(x, y)` that takes in two arrays and returns the Pearson correlation coefficient. We will then use this function to compute it for the petal lengths and widths of $I.\\ versicolor$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_r(x, y):\n",
    "    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n",
    "    # Compute correlation matrix: corr_mat\n",
    "    corr_mat = np.corrcoef(x,y)\n",
    "\n",
    "    # Return entry [0,1]\n",
    "    return corr_mat[0,1]\n",
    "\n",
    "# Compute Pearson correlation coefficient for I. versicolor: r\n",
    "r = pearson_r(versicolor_petal_length, versicolor_petal_width)\n",
    "\n",
    "# Print the result\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking probabilistically-- Discrete variables\n",
    "\n",
    "Statistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data. We will think probabilistically about discrete quantities: those that can only take certain values, like integers.\n",
    "\n",
    "## Probabilistic logic and statistical inference\n",
    "\n",
    "### the goal of statistical inference\n",
    "\n",
    "* To draw probabilistic conclusions about what we might expect if we collected the same data again.\n",
    "* To draw actionable conclusions from data.\n",
    "* To draw more general conclusions from relatively few data or observations.\n",
    "\n",
    "> Note: Statistical inference involves taking your data to probabilistic conclusions about what you would expect if you took even more data, and you can make decisions based on these conclusions.\n",
    "\n",
    "### Why we use the probabilistic language in statistical inference\n",
    "\n",
    "* Probability provides a measure of uncertainty and this is crucial because we can quantify what we might expect if the data were acquired again.\n",
    "* Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary. We need probability to say how data might vary if acquired again.\n",
    "\n",
    "> Note: Probabilistic language is in fact very precise. It precisely describes uncertainty.\n",
    "\n",
    "## Random number generators and hacker statistics\n",
    "\n",
    "> ### Hacker statistics\n",
    "- Uses simulated repeated measurements to compute\n",
    "probabilities.\n",
    "\n",
    "> ### The np.random module\n",
    "- Suite of functions based on random number generation\n",
    "- `np.random.random()`: draw a number between $0$ and $1$\n",
    " \n",
    "> ### Bernoulli trial\n",
    "● An experiment that has two options,\n",
    "\"success\" (True) and \"failure\" (False).\n",
    "\n",
    "> ### Random number seed\n",
    "- Integer fed into random number generating algorithm\n",
    "- Manually seed random number generator if you need reproducibility\n",
    "- Specified using `np.random.seed()`\n",
    "\n",
    "> ### Hacker stats probabilities\n",
    "- Determine how to simulate data\n",
    "- Simulate many many times\n",
    "- Probability is approximately fraction of trials with the outcome of interest\n",
    "\n",
    "### Generating random numbers using the np.random module\n",
    "\n",
    "we'll generate lots of random numbers between zero and one, and then plot a histogram of the results. If the numbers are truly random, all bars in the histogram should be of (close to) equal height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random number generator\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize random numbers: random_numbers\n",
    "random_numbers = np.empty(100000)\n",
    "\n",
    "# Generate random numbers by looping over range(100000)\n",
    "for i in range(100000):\n",
    "    random_numbers[i] = np.random.random()\n",
    "\n",
    "# Plot a histogram\n",
    "_ = plt.hist(random_numbers, bins=316, histtype=\"step\", density=True)\n",
    "_ = plt.xlabel(\"random numbers\")\n",
    "_ = plt.ylabel(\"counts\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The histogram is almost exactly flat across the top, indicating that there is equal chance that a randomly-generated number is in any of the bins of the histogram.\n",
    "\n",
    "### The np.random module and Bernoulli trials\n",
    "\n",
    "> Tip: You can think of a Bernoulli trial as a flip of a possibly biased coin. Each coin flip has a probability $p$ of landing heads (success) and probability $1−p$ of landing tails (failure). \n",
    "\n",
    "We will write a function to perform `n` Bernoulli trials, `perform_bernoulli_trials(n, p)`, which returns the number of successes out of `n` Bernoulli trials, each of which has probability $p$ of success. To perform each Bernoulli trial, we will use the `np.random.random()` function, which returns a random number between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bernoulli_trials(n, p):\n",
    "    \"\"\"Perform n Bernoulli trials with success probability p\n",
    "    and return number of successes.\"\"\"\n",
    "    # Initialize number of successes: n_success\n",
    "    n_success = False\n",
    "\n",
    "    # Perform trials\n",
    "    for i in range(n):\n",
    "        # Choose random number between zero and one: random_number\n",
    "        random_number = np.random.random()\n",
    "\n",
    "        # If less than p, it's a success so add one to n_success\n",
    "        if random_number < p:\n",
    "            n_success += 1\n",
    "\n",
    "    return n_success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many defaults might we expect?\n",
    "\n",
    "Let's say a bank made 100 mortgage loans. It is possible that anywhere between $0$ and $100$ of the loans will be defaulted upon. We would like to know the probability of getting a given number of defaults, given that the probability of a default is $p = 0.05$. To investigate this, we will do a simulation. We will perform 100 Bernoulli trials using the `perform_bernoulli_trials()` function and record how many defaults we get. Here, a success is a default. \n",
    "\n",
    "> Important: Remember that the word \"success\" just means that the Bernoulli trial evaluates to True, i.e., did the loan recipient default? \n",
    "\n",
    "You will do this for another $100$ Bernoulli trials. And again and again until we have tried it $1000$ times. Then, we will plot a histogram describing the probability of the number of defaults.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random number generator\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize the number of defaults: n_defaults\n",
    "n_defaults = np.empty(1000)\n",
    "\n",
    "# Compute the number of defaults\n",
    "for i in range(1000):\n",
    "    n_defaults[i] = perform_bernoulli_trials(100, 0.05)\n",
    "\n",
    "\n",
    "# Plot the histogram with default number of bins; label your axes\n",
    "_ = plt.hist(n_defaults, density=True)\n",
    "_ = plt.xlabel('number of defaults out of 100 loans')\n",
    "_ = plt.ylabel('probability')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: This is actually not an optimal way to plot a histogram when the results are known to be integers. We will revisit this\n",
    "\n",
    "### Will the bank fail?\n",
    "\n",
    "If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECDF: x, y\n",
    "x,y = ecdf(n_defaults)\n",
    "\n",
    "# Plot the ECDF with labeled axes\n",
    "_ = plt.plot(x,y, marker=\".\", linestyle=\"none\")\n",
    "_ = plt.xlabel(\"number of defaults\")\n",
    "_ = plt.ylabel(\"ECDF\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\n",
    "n_lose_money = np.sum(n_defaults >= 10)\n",
    "\n",
    "# Compute and print probability of losing money\n",
    "print('Probability of losing money =', n_lose_money / len(n_defaults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: we most likely get 5/100 defaults. But we still have about a 2% chance of getting 10 or more defaults out of 100 loans.\n",
    "\n",
    "## Probability distributions and stories: The Binomial distribution\n",
    "\n",
    "> ### Probability mass function (PMF)\n",
    "- The set of probabilities of discrete outcomes\n",
    "\n",
    "> ### Probability distribution\n",
    "- A mathematical description of outcomes\n",
    "\n",
    "> ### Discrete Uniform distribution: the story\n",
    "- The outcome of rolling a single fair die is Discrete Uniformly distributed.\n",
    "\n",
    "> ### Binomial distribution: the story\n",
    "- The number $r$ of successes in $n$ Bernoulli trials with\n",
    "probability $p$ of success, is Binomially distributed\n",
    "- The number $r$ of heads in $4$ coin flips with probability\n",
    "$0.5$ of heads, is Binomially distributed\n",
    "\n",
    "### Sampling out of the Binomial distribution\n",
    "\n",
    "We will compute the probability mass function for the number of defaults we would expect for $100$ loans as in the last section, but instead of simulating all of the Bernoulli trials, we will perform the sampling using `np.random.binomial()`{% fn 1 %}.\n",
    "\n",
    "> Note: This is identical to the calculation we did in the last set of exercises using our custom-written `perform_bernoulli_trials()` function, but far more computationally efficient. \n",
    "\n",
    "Given this extra efficiency, we will take $10,000$ samples instead of $1000$. After taking the samples, we will plot the CDF. This CDF that we are plotting is that of the Binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 10,000 samples out of the binomial distribution: n_defaults\n",
    "\n",
    "n_defaults = np.random.binomial(100, 0.05, size=10000)\n",
    "# Compute CDF: x, y\n",
    "x,y = ecdf(n_defaults)\n",
    "\n",
    "# Plot the CDF with axis labels\n",
    "_ = plt.plot(x,y, marker=\".\", linestyle=\"-\")\n",
    "_ = plt.xlabel(\"number of defaults out of 100 loans\")\n",
    "_ = plt.ylabel(\"CDF\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: If you know the story, using built-in algorithms to directly sample out of the distribution is ***much*** faster.\n",
    "\n",
    "### Plotting the Binomial PMF\n",
    "\n",
    "> Warn: plotting a nice looking PMF requires a bit of matplotlib trickery that we will not go into here.\n",
    "\n",
    "we will plot the PMF of the Binomial distribution as a histogram. The trick is setting up the edges of the `bins` to pass to `plt.hist()` via the `bins` keyword argument. We want the bins centered on the integers. So, the edges of the bins should be $-0.5, 0.5, 1.5, 2.5, ...$ up to `max(n_defaults) + 1.5`. We can generate an array like this using `np.arange() `and then subtracting `0.5` from the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bin edges: bins\n",
    "bins = np.arange(0, max(n_defaults) + 1.5) - 0.5\n",
    "\n",
    "# Generate histogram\n",
    "_ = plt.hist(n_defaults, density=True, bins=bins)\n",
    "\n",
    "# Label axes\n",
    "_ = plt.xlabel(\"number of defaults out of 100 loans\")\n",
    "_ = plt.ylabel(\"probability\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson processes and the Poisson distribution\n",
    "\n",
    "> ### Poisson process\n",
    "- The timing of the next event is completely independent of when the previous event happened\n",
    "\n",
    "> ### Examples of Poisson processes\n",
    "- Natural births in a given hospital\n",
    "- Hit on a website during a given hour\n",
    "- Meteor strikes\n",
    "- Molecular collisions in a gas\n",
    "- Aviation incidents\n",
    "- Buses in Poissonville\n",
    "\n",
    "> ### Poisson distribution\n",
    "- The number $r$ of arrivals of a Poisson process in a\n",
    "given time interval with average rate of $λ$ arrivals\n",
    "per interval is Poisson distributed.\n",
    "- The number r of hits on a website in one hour with\n",
    "an average hit rate of 6 hits per hour is Poisson\n",
    "distributed.\n",
    "\n",
    "> ### Poisson Distribution\n",
    "- Limit of the Binomial distribution for low\n",
    "probability of success and large number of trials.\n",
    "- That is, for rare events.\n",
    "\n",
    "### Relationship between Binomial and Poisson distributions\n",
    "\n",
    "> Important: Poisson distribution is a limit of the Binomial distribution for rare events.\n",
    "\n",
    "> Tip: Poisson distribution with arrival rate equal to $np$ approximates a Binomial distribution for $n$ Bernoulli trials with probability $p$ of success (with $n$ large and $p$ small). Importantly, the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution.\n",
    "\n",
    "Let's explore these two distributions computationally. We will compute the mean and standard deviation of samples from a Poisson distribution with an arrival rate of $10$. Then, we will compute the mean and standard deviation of samples from a Binomial distribution with parameters $n$ and $p$ such that $np = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 10,000 samples out of Poisson distribution: samples_poisson\n",
    "samples_poisson = np.random.poisson(10, size=10000)\n",
    "\n",
    "# Print the mean and standard deviation\n",
    "print('Poisson:     ', np.mean(samples_poisson),\n",
    "                       np.std(samples_poisson))\n",
    "\n",
    "# Specify values of n and p to consider for Binomial: n, p\n",
    "n = [20, 100, 1000]\n",
    "p = [.5, .1, .01]\n",
    "\n",
    "# Draw 10,000 samples for each n,p pair: samples_binomial\n",
    "for i in range(3):\n",
    "    samples_binomial = np.random.binomial(n[i],p[i], size=10000)\n",
    "\n",
    "    # Print results\n",
    "    print('n =', n[i], 'Binom:', np.mean(samples_binomial),\n",
    "                                 np.std(samples_binomial))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The means are all about the same, which can be shown to be true by doing some pen-and-paper work. The standard deviation of the Binomial distribution gets closer and closer to that of the Poisson distribution as the probability $p$ gets lower and lower.\n",
    "\n",
    "### Was 2015 anomalous?\n",
    "\n",
    "In baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. The ECDF of the number of no-hitters in a season is shown to the right. The  probability distribution that would be appropriate to describe the number of no-hitters we would expect in a given season? is Both Binomial and Poisson, though Poisson is easier to model and compute.\n",
    "\n",
    "> Important: When we have rare events (low $p$, high $n$), the Binomial distribution is Poisson. This has a single parameter, the mean number of successes per time interval, in our case the mean number of no-hitters per season.\n",
    "\n",
    "1990 and 2015 featured the most no-hitters of any season of baseball (there were seven). Given that there are on average $\\frac{251}{115}$ no-hitters per season, what is the probability of having seven or more in a season? Let's find out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 10,000 samples out of Poisson distribution: n_nohitters\n",
    "n_nohitters = np.random.poisson(251/115, size=10000)\n",
    "\n",
    "# Compute number of samples that are seven or greater: n_large\n",
    "n_large = np.sum(n_nohitters >= 7)\n",
    "\n",
    "# Compute probability of getting seven or more: p_large\n",
    "p_large = n_large/10000\n",
    "\n",
    "# Print the result\n",
    "print('Probability of seven or more no-hitters:', p_large)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The result is about $0.007$. This means that it is not that improbable to see a 7-or-more no-hitter season in a century. We have seen two in a century and a half, so it is not unreasonable.\n",
    "\n",
    "# Thinking probabilistically-- Continuous variables\n",
    "\n",
    "It’s time to move onto continuous variables, such as those that can take on any fractional value. Many of the principles are the same, but there are some subtleties. We will be speaking the probabilistic language needed to launch into the inference techniques.\n",
    "\n",
    "## Probability density functions\n",
    "\n",
    "> ### Continuous variables\n",
    "- Quantities that can take any value, not just\n",
    "discrete values\n",
    "\n",
    "> ### Probability density function (PDF)\n",
    "- Continuous analog to the PMF\n",
    "● Mathematical description of the relative likelihood\n",
    "of observing a value of a continuous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{'>Note: For this exercise and all going forward, the random number generator is pre-seeded for you (with `np.random.seed(42))` to save you typing that each time.' | fndetail:1 }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
